"""
This module creates generation capacities to generator buses. The idea is the same as the load settings in `load_allocator.py`.
"""

import numpy as np
import networkx as nx
import math
from typing import List, Dict, Tuple, Optional
from .reference_data import get_reference_stats

class CapacityAllocator:
    """
    Assigns generation capacities (PgMax) to generator buses in the grid.
    """

    def __init__(self, graph: nx.Graph, ref_sys_id: int = 1):
        """
        Args:
            graph: The networkx graph with 'bus_type' attributes.
            ref_sys_id: ID of reference system statistics to use (1, 2, or 3).
                        Use 0 for the previous heuristic (diagonal correlation) default.
        """
        self.graph = graph
        self.ref_sys_id = ref_sys_id
        self.nodes = list(graph.nodes())
        self.n_nodes = len(self.nodes)
        
        # Identify Generator Buses (Assuming 'bus_type' attribute exists)
        # 1=Gen, 2=Load, 3=Conn
        # If attribute missing, defaults to empty list (user must run BusTypeAllocator first)
        self.gen_buses = [n for n, d in graph.nodes(data=True) if d.get('bus_type') == 'Gen']
        self.n_gen = len(self.gen_buses)

    def _generate_heuristic_tab_2d(self) -> np.ndarray:
        """
        Generates a heuristic 14x14 correlation table (previous default).
        This assumes a general positive correlation between node degree and capacity
        (diagonal bias), without relying on specific reference system data.
        """
        # Create a matrix with some diagonal weight to simulate correlation
        # Rows = Capacity Classes (low to high), Cols = Degree Classes (low to high)
        matrix = np.zeros((14, 14))
        for r in range(14):
            for c in range(14):
                # Distance from diagonal
                dist = abs(r - c)
                # higher probability near diagonal
                matrix[r, c] = np.exp(-0.5 * dist) 
        
        # Normalize to sum to 1
        return matrix / np.sum(matrix)

    def _get_default_tab_2d(self) -> np.ndarray:
        """
        Returns the Tab_2D_Pgmax from the selected reference system.
        This represents the joint probability of (Node Degree Class, Capacity Class).
        """
        # Option 0: Use the heuristic generator (previous default)
        if self.ref_sys_id == 0:
            return self._generate_heuristic_tab_2d()

        try:
            stats = get_reference_stats(self.ref_sys_id)
            return stats['Tab_2D_Pgmax']
        except ValueError as e:
            print(f"Warning: {e} Defaulting to Reference System 1.")
            stats = get_reference_stats(1)
            return stats['Tab_2D_Pgmax']

    def _initial_generation_distribution(self, total_gen: float) -> Tuple[np.ndarray, float]:
        """
        Generates generation capacities based on empirical distributions.
        99% exponential, 1% super large (uniform).
        """
        if self.n_gen == 0:
            return np.array([]), 0.0

        # Mean capacity per generator
        mu = total_gen / self.n_gen
        
        # Base distribution: Exponential
        p_caps = np.random.exponential(scale=mu, size=self.n_gen)
        
        # Super large capacities (approx 1%)
        n_super = int(round(0.01 * self.n_gen))
        if n_super > 0:
            max_p = np.max(p_caps)
            # Uniform random [1*max, 3*max] (based on: max(P) + 2*rand*max(P))
            p_super = max_p + (2.0 * np.random.rand(n_super) * max_p)
            
            # Replace random indices with super large values
            indices = np.random.choice(self.n_gen, size=n_super, replace=False)
            p_caps[indices] = p_super

        # Scaling check
        current_sum = np.sum(p_caps)
        if current_sum > 1.05 * total_gen or current_sum < 0.90 * total_gen:
            p_caps = p_caps * (total_gen / current_sum)
            
        max_r_pgmax = np.max(p_caps)
        normalized_r_pgmax = p_caps / max_r_pgmax
        
        return p_caps, max_r_pgmax, normalized_r_pgmax

    def _assignment_logic(self, norm_deg_pairs: np.ndarray, norm_caps: np.ndarray, tab_2d: np.ndarray) -> np.ndarray:
        """
        The core binning and assignment logic (Algorithm described in 'Assignment' function).
        
        Args:
            norm_deg_pairs: (N_gen x 2) array of [NodeID, NormDegree]
            norm_caps: (N_gen x 1) array of normalized capacities
            tab_2d: 14x14 probability matrix
            
        Returns:
            Assigned capacities array (N_gen x 3): [NodeID, NormDegree, NormCapacity]
        """
        ng = len(norm_caps)
        
        # 1. Calculate target counts for the 2D table
        tab_2d_ng = np.round(tab_2d * ng).astype(int)
        
        # Fix rounding errors to match exactly ng
        current_sum = np.sum(tab_2d_ng)
        diff = ng - current_sum
        
        if diff != 0:
            # Add/Sub from the max element to assume least disruption
            # Flatten find max linear index
            flat_idx = np.argmax(tab_2d_ng)
            # unravel
            r_idx, c_idx = np.unravel_index(flat_idx, tab_2d_ng.shape)
            tab_2d_ng[r_idx, c_idx] += diff

        # 2. Binning Targets
        # Column sums = targets for Degree bins
        n_k_targets = np.sum(tab_2d_ng, axis=0) # 1x14
        # Row sums = targets for Capacity bins
        n_g_targets = np.sum(tab_2d_ng, axis=1) # 1x14
        
        # 3. Sort and Bin Nodes by Degree
        # Sort by degree (column 1)
        sorted_nodes = norm_deg_pairs[norm_deg_pairs[:, 1].argsort()]
        
        k_bins = []
        current_idx = 0
        for count in n_k_targets:
            end_idx = current_idx + count
            k_bins.append(sorted_nodes[current_idx:end_idx].copy())
            current_idx = end_idx
            
        # 4. Sort and Bin Capacities
        # Sort capacities ascending
        sorted_caps = np.sort(norm_caps)
        
        g_bins = []
        current_idx = 0
        for count in n_g_targets:
            end_idx = current_idx + count
            # Keep as list for easy popping
            g_bins.append(list(sorted_caps[current_idx:end_idx]))
            current_idx = end_idx

        # 5. Assignment Loop (The 2D allocation)
        # Iterate Columns (Degree bins) 1..14
        for k_idx in range(14): # kk
            # Iterate Rows (Capacity bins) 14..1 (High to Low)
            # MATLAB: for gg=14:-1:1
            for g_idx in range(13, -1, -1): # gg
                count = tab_2d_ng[g_idx, k_idx]
                if count > 0:
                    # Take 'count' capacities from G_bin[g_idx]
                    # Since G_bin is sorted, random sampling minimizes bias within the bin
                    caps_to_assign = []
                    source_bin = g_bins[g_idx]
                    
                    # Safety check if bin ran out due to rounding weirdness (shouldn't happen with logic above)
                    take = min(count, len(source_bin))
                    
                    for _ in range(take):
                        # Random sample without replacement strategy from the bin
                        # pop(0) takes smallest, pop(-1) takes largest in that bin
                        # MATLAB uses datasample which is random
                        rand_i = np.random.randint(0, len(source_bin))
                        caps_to_assign.append(source_bin.pop(rand_i))
                    
                    # Assign to nodes in K_bin[k_idx]
                    # We need to fill the 'NormCapacity' column (index 2 in final output, but we construct it)
                    # K_bin currently: [NodeID, NormDeg]
                    # We need to find 'take' nodes in K_bin that don't have capacity yet
                    # Actually, K_bins are partitioned disjointly. We just fill them up.
                    
                    # We add a 3rd column for Capacity to k_bins data structures
                    # Initialize if not present
                    if k_bins[k_idx].shape[1] < 3:
                        # Append column of -1s
                        k_bins[k_idx] = np.hstack((k_bins[k_idx], np.full((len(k_bins[k_idx]), 1), -1.0)))
                        
                    # Find empty slots (-1)
                    empty_slots = np.where(k_bins[k_idx][:, 2] == -1.0)[0]
                    
                    # Fill 'take' slots
                    fill_indices = empty_slots[:take]
                    k_bins[k_idx][fill_indices, 2] = caps_to_assign

        # 6. Reassemble
        # Stack all K bins
        final_list = []
        for bin_arr in k_bins:
            if bin_arr.shape[1] == 3: # valid bin
                final_list.append(bin_arr)
            else:
                # Bin might be empty or untouched?
                pass
                
        if not final_list:
            return np.array([])
            
        gen_setting = np.vstack(final_list)
        return gen_setting

    def allocate(self, tab_2d: Optional[np.ndarray] = None) -> Dict[int, float]:
        """
        Main execution method.
        
        Args:
            tab_2d: Optional 14x14 probability matrix. If None, uses default based on ref_sys_id.
            
        Returns:
            Dictionary mapping Generator Node ID -> Capacity (PgMax)
        """
        if self.n_gen == 0:
            print("No generator buses found. Skipping capacity allocation.")
            return {}

        if tab_2d is None:
            tab_2d = self._get_default_tab_2d()

        # 1. Calculate Total Generation
        # Total_Generation = 10^(-0.21*(log10(N))^2+2.06*log10(N)+0.66)
        log10_n = np.log10(self.n_nodes)
        exponent = -0.21 * (log10_n**2) + 2.06 * log10_n + 0.66
        total_gen = 10**exponent
        
        ref_label = "Heuristic (Default)" if self.ref_sys_id == 0 else f"Reference System {self.ref_sys_id}"
        print(f"Allocating Capacity for {self.n_gen} generators.")
        print(f"Total System Capacity Target: {total_gen:.2f} MW using {ref_label}")

        # 2. Get Node Degrees for Generators
        gen_degrees = []
        for n in self.gen_buses:
            gen_degrees.append([n, self.graph.degree(n)])
        gen_degrees = np.array(gen_degrees)
        
        max_node_degree = np.max(gen_degrees[:, 1])
        
        # Normalized Node Degree [NodeID, NormDeg]
        norm_gen_degrees = gen_degrees.copy().astype(float)
        norm_gen_degrees[:, 1] = norm_gen_degrees[:, 1] / max_node_degree

        # 3. Initial Generation Distribution
        # We need actual values P_caps AND normalized values
        p_caps_actual, max_r_pgmax, norm_r_pgmax = self._initial_generation_distribution(total_gen)
        
        # 4. Assignment
        # Returns [NodeID, NormDeg, NormCap]
        norm_assignment = self._assignment_logic(norm_gen_degrees, norm_r_pgmax, tab_2d)
        
        # 5. Denormalize
        # Map NodeID -> NormCap * MaxCap
        final_allocation = {}
        for row in norm_assignment:
            node_id = int(row[0])
            norm_cap = row[2]
            actual_cap = norm_cap * max_r_pgmax
            final_allocation[node_id] = actual_cap
            
        return final_allocation